#+TITLE: Notes for CL-BAYESIAN
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil tags:not-in-toc author:nil
#+OPTIONS:   H:3 num:nil toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:tl creator:nil

Sketches of proofs and reminders for the algorithms behind some
functions.  Most of these results are entirely standard, I am including them here mainly to fix notation.  For distributions, I am using the same parametrization as Gelman et al (2004) [BDA].

* UDDU decomposition

$$A=UD^2U^T$$
where $U$ is unitary (ie $UU^T=I$ and $U$ is square) and $D$ is a diagonal matrix.

** Inverting it is really cheap:
$$A^{-1} = (UD^2U^T)^{-1} = {U^T}^{-1} D^{-2} U^{-1} = U D^{-2} U^T$$

** Adding a Hermitian matrix $H=LL^T$:
$$A+H = XX^T \qquad\text{where}\qquad X=\begin{bmatrix}UD & L\end{bmatrix}$$
Using SVD,
$$X=\tilde U\tilde D\tilde V^T$$
so
$$A+H=\tilde U \tilde D^2 \tilde U^T$$
Note that $\tilde V\tilde V^T=I$ and thus $\tilde V^T$ is not needed from the SVD.

** Another way of adding a Hermitian matrix:
$$A+H=U(D^2+U^TLL^TU)U^T=UYY^TU^T \qquad\text{where}\qquad Y=\begin{bmatrix}D & U^TL\end{bmatrix}$$
Then using SVD,
$$Y=\hat U\hat D\hat V^T$$
so
$$A+H=U \hat U \hat D^2 \hat U^T U^T$$
Again, we don't need to calculate $\hat V^T$.

* Simple Bayesian models

These models can be used on their own, but they are meant to be building blocks for Gibbs samplers.

** Univariate normal error

$$\epsilon_i \sim N(0,v), \text{iid}, n=1,\dots,n$$
where $v=\sigma^2$ is the variance.  The likelihood is 
$$p(\epsilon \mid v) \propto v^{-n/2} e^{s/(2v)}\qquad\text{where}\qquad s=\sum_i \epsilon_i^2$$
and with an $\text{inverse-gamma}(\alpha_0,\beta_0)$ conjugate prior,
the posterior is
$$(v\mid\epsilon) \sim \text{inverse-gamma}(\alpha_0+n/2,\beta_0+s/2)$$

** Univariate normal model

$$\epsilon_i \sim N(m,v), \text{iid}, n=1,\dots,n$$
The likelihood is
$$
